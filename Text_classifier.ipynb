{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBtif9-0a45h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"train.txt\",delimiter=';',names=['sentence','label'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nF_dP7ggbKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.value_counts('label')"
      ],
      "metadata": {
        "id": "s5N8ozX5bfS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "df['label']=le.fit_transform(df['label'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nR0eYjIKboV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create class weights for labels\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(df['label']),\n",
        "    y=df['label']\n",
        ")\n",
        "\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(class_weights_dict)\n"
      ],
      "metadata": {
        "id": "XrSyyRRvbvlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "WzjIblvye04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokanizer=Tokenizer(num_words=10000)\n",
        "tokanizer.fit_on_texts(df['sentence'])\n",
        "sequences=tokanizer.texts_to_sequences(df['sentence'])\n"
      ],
      "metadata": {
        "id": "9MfEJuYae9Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokanizer.word_counts)"
      ],
      "metadata": {
        "id": "fLyQ8xnOfNj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_df=pad_sequences(sequences,maxlen=100,padding='post')\n",
        "tr_y=to_categorical(df['label'])"
      ],
      "metadata": {
        "id": "g7Iv7ZqqfcDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokanizer.word_index)+1\n",
        "embedding_dim=128\n",
        "max_length=100\n",
        "num_classes=6"
      ],
      "metadata": {
        "id": "3L-oBScxf0Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout,Bidirectional,GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "3eBcRaCRgNdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes,activation='softmax'))"
      ],
      "metadata": {
        "id": "Gm_O-oBWiBnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True))) # Added return_sequences=True\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes,activation='softmax'))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UCb3cS4LjmRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make combile and use class_weight\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(tr_df, tr_y, epochs=15, batch_size=32, validation_split=0.1, class_weight=class_weights_dict, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "HC3TqiimjLym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read test.txt and split the sentence and label then convert label by labelencoder and predict the sentense label then get the accurecy\n",
        "\n",
        "df_test = pd.read_csv(\"test.txt\", delimiter=';', names=['sentence', 'label'])\n",
        "df_test['label'] = le.transform(df_test['label'])\n",
        "test_sequences = tokanizer.texts_to_sequences(df_test['sentence'])\n",
        "test_df = pad_sequences(test_sequences, maxlen=100, padding='post')\n",
        "test_y = to_categorical(df_test['label'])\n",
        "\n",
        "loss, accuracy = model.evaluate(test_df, test_y)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "y_pred = model.predict(test_df)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(test_y, axis=1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "yDTCGNYCkHwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read test.txt and split the sentence and label then convert label by labelencoder and predict the sentense label then get the accurecy\n",
        "\n",
        "df_test = pd.read_csv(\"val.txt\", delimiter=';', names=['sentence', 'label'])\n",
        "df_test['label'] = le.transform(df_test['label'])\n",
        "test_sequences = tokanizer.texts_to_sequences(df_test['sentence'])\n",
        "test_df = pad_sequences(test_sequences, maxlen=100, padding='post')\n",
        "test_y = to_categorical(df_test['label'])\n",
        "\n",
        "loss, accuracy = model.evaluate(test_df, test_y)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "y_pred = model.predict(test_df)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(test_y, axis=1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "GLx7pzDsk0tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokanizer, f)\n"
      ],
      "metadata": {
        "id": "XfyzF34PrVfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming your model is named `model`\n",
        "# Instead of using pickle, use model.save to save it in HDF5 format\n",
        "model.save('model.h5')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Rsdcg_ImtVfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "dedioi5oto5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: predict the feeling of \"happy\" convert it to the word\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf # Import tensorflow\n",
        "import pickle\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pkl', 'rb') as f:\n",
        "    loaded_tokenizer = pickle.load(f)\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('model.h5') # Changed to load_model from tf.keras.models and corrected file extension to .h5\n",
        "\n",
        "# Input sentence\n",
        "input_sentence = \"i didnt feel humiliated\t\"\n",
        "\n",
        "# Convert the sentence to a sequence of integers\n",
        "sequence = loaded_tokenizer.texts_to_sequences([input_sentence])\n",
        "\n",
        "# Pad the sequence\n",
        "padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')\n",
        "\n",
        "# Make predictions\n",
        "prediction = loaded_model.predict(padded_sequence)\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "# Assuming you have a label encoder named `le`\n",
        "# Convert the predicted class index to the original label\n",
        "predicted_label = le.inverse_transform([predicted_class_index])[0]\n",
        "\n",
        "print(f\"The predicted feeling is: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "INwx905_wFUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hUuNMYSwv2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate any open tunnels in case they're already running\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your authtoken\n",
        "ngrok.set_auth_token(\"2j5sVDMLHk5H7LWu2O9nr0K7DeG_5r5yLnFAaauAjVjDfoWmP\") # Replace YOUR_AUTHTOKEN with the token you copied\n",
        "\n",
        "# Create a new tunnel on port 8501 (default Streamlit port)\n",
        "# Pass the port as an integer and explicitly specify the protocol\n",
        "public_url = ngrok.connect(8501, proto=\"http\")  # Changed to integer port and added protocol\n",
        "public_url"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i9fm9LLCwoiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O- ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "id": "cMidJGC7xQih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make dictionary for the label to delabel it\n",
        "\n",
        "label_to_delabel = {label: delabel for label, delabel in zip(df['label'], le.inverse_transform(df['label']))}\n",
        "print(label_to_delabel)"
      ],
      "metadata": {
        "id": "gbzvFr2I0QSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8500"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2a1cFEPiwyd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make dataframe from 2sample from enevry label\n",
        "\n",
        "# Create an empty list to store the samples\n",
        "samples_per_label = []\n",
        "\n",
        "# Iterate through unique labels in the DataFrame\n",
        "for label in df['label'].unique():\n",
        "  # Get two random samples for the current label\n",
        "  label_samples = df[df['label'] == label].sample(n=2, random_state=42)  # Adjust 'n' for the number of samples per label\n",
        "  samples_per_label.append(label_samples)\n",
        "\n",
        "# Concatenate the samples into a new DataFrame\n",
        "new_df = pd.concat(samples_per_label)\n",
        "\n",
        "new_df"
      ],
      "metadata": {
        "id": "3a8fBq2A2YQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[11433,0]"
      ],
      "metadata": {
        "id": "pw6_9iKf3CUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "df.iloc[15428,0]  # Access the row with index 11433\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yP7vXgEZ3XSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[4989,0]"
      ],
      "metadata": {
        "id": "vue5umWG3x5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[14343,0]"
      ],
      "metadata": {
        "id": "eJPrbvaE33Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[14133,0]"
      ],
      "metadata": {
        "id": "zW88Itzv36Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[4996,0]"
      ],
      "metadata": {
        "id": "bjteYyuO39uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[10462,0]"
      ],
      "metadata": {
        "id": "QNQWDJzt4Etu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}